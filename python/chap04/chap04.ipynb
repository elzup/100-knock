{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章: 形態素解析\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
    "\n",
    "なお，問題37, 38, 39はmatplotlibもしくはGnuplotを用いるとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# $ mecab neko.txt > neko.txt.mecab\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. 形態素解析結果の読み込み\n",
    "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('neko.txt.mecab', 'r', encoding='utf-8') as f:\n",
    "    neko_mecab = f.readlines()\n",
    "\n",
    "xstrip = lambda x: x.strip(\"\\n\")\n",
    "is_not_eos = lambda x: x != \"EOS\"\n",
    "\n",
    "neko_lines = list(filter(is_not_eos, map(xstrip, neko_mecab)))\n",
    "\n",
    "def convert_nlpobj(line):\n",
    "    surface, props = line.split(\"\\t\")\n",
    "    pos, pos1, _, _, _, _, base, _, _ = (props.split(\",\") + [None, None])[:9]\n",
    "    return {\n",
    "        'surface': surface,\n",
    "        'base': base,\n",
    "        'pos': pos,\n",
    "        'pos1': pos1 }\n",
    "map_surface = lambda x: x['surface']\n",
    "map_base = lambda x: x['base']\n",
    "map_pos = lambda x: x['pos']\n",
    "map_pos1 = lambda x: x['pos1']\n",
    "neko_lib = list(map(convert_nlpobj, neko_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. 動詞\n",
    "動詞の表層形をすべて抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れ,つか,し,泣い,し,いる,始め,見,聞く,捕え,煮,食う,思わ,載せ,られ,持ち上げ,られ,し,あっ,落ちつい...\n"
     ]
    }
   ],
   "source": [
    "verb_filter = lambda x: x['pos'] == '動詞'\n",
    "verbs_surfaces = list(map(map_surface, filter(verb_filter, neko_lib)))\n",
    "print(\",\".join(verbs_surfaces[:20]) + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32. 動詞の原形\n",
    "動詞の原形をすべて抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる,つく,する,泣く,する,いる,始める,見る,聞く,捕える,煮る,食う,思う,載せる,られる,持ち上げる,られる,する,ある,落ちつく...\n"
     ]
    }
   ],
   "source": [
    "verbs_bases = list(map(map_base, filter(verb_filter, neko_lib)))\n",
    "print(\",\".join(verbs_bases[:20]) + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33. サ変名詞\n",
    "サ変接続の名詞をすべて抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "記憶,装飾,突起,咽,運転,記憶,我慢,餓死,訪問,遭遇,記憶,珍重,同居,観察,断言,同衾,尊敬,剿滅,憤慨,失敗...\n"
     ]
    }
   ],
   "source": [
    "noun_filter = lambda x: x['pos'] == '名詞'\n",
    "is_suru = lambda x:  x['base'] == 'する'\n",
    "sahen_noun_filter = lambda x, nex: noun_filter(x) and is_suru(nex)\n",
    "\n",
    "sahen_nouns = []\n",
    "for i, x in enumerate(neko_lib):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if sahen_noun_filter(neko_lib[i - 1], x):\n",
    "        sahen_nouns.append(neko_lib[i - 1])\n",
    "        \n",
    "print(\",\".join(list(map(map_base, sahen_nouns[:20]))) + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34. 「AのB」\n",
    "2つの名詞が「の」で連結されている名詞句を抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-42d3985b97c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnonoun_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneko_lib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mneko_lib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'の'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneko_lib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "nonoun_phrase = []\n",
    "for i, x in enumerate(neko_lib):\n",
    "    if neko_lib['base'] != 'の':\n",
    "        continue\n",
    "    pre, cur, nex = neko_lib[i - 1: i + 2]\n",
    "    if not noun_filter(pre) or noun_filter(nex):\n",
    "        continue\n",
    "    nonoun_phrase.append(pre['surface'] + cur['surface'] + nex['surface'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35. 名詞の連接\n",
    "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36. 単語の出現頻度\n",
    "文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 37. 頻度上位10語\n",
    "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 38. ヒストグラム\n",
    "単語の出現頻度のヒストグラム（横軸に出現頻度，縦軸に出現頻度をとる単語の種類数を棒グラフで表したもの）を描け．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39. Zipfの法則\n",
    "単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
